{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing Publications Automatic Ingest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from hashlib import md5\n",
    "from pathlib import Path\n",
    "from shutil import copy2, rmtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     1,
     33,
     47,
     60,
     64
    ]
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_formatted_extension(from_extension, remediate=False):\n",
    "    '''\n",
    "    -- Purpose --\n",
    "    Returns an extension that:\n",
    "    1. has a period in the front\n",
    "    2. Optional: is lower-case\n",
    "    3. Optional: return jpeg as jpg and tiff as tif\n",
    "\n",
    "    -- Arguments --\n",
    "    from_extension: type=string; file extension with or without a '.'\n",
    "\n",
    "    -- Returns --\n",
    "    formatted_extension: type=string; formatted extension\n",
    "    '''\n",
    "    # make sure there's a period at the front of the extension\n",
    "    if from_extension.startswith('.'):  # do nothing\n",
    "        formatted_extension = from_extension\n",
    "    else:  # add a period\n",
    "        formatted_extension = f'.{from_extension}'\n",
    "\n",
    "    # make it lower-case\n",
    "    if remediate:\n",
    "        formatted_extension = formatted_extension.lower()\n",
    "        # hard-coded alterations for jpeg and tiff\n",
    "        if formatted_extension == '.jpeg':\n",
    "            formatted_extension = '.jpg'\n",
    "        elif formatted_extension == '.tiff':\n",
    "            formatted_extension = '.tif'\n",
    "\n",
    "    return formatted_extension\n",
    "\n",
    "\n",
    "def get_season_code(season):\n",
    "    seasons_dict = {'spring': '21',\n",
    "                    'summer': '22',\n",
    "                    'fall': '23',\n",
    "                    'winter': '24'\n",
    "        \n",
    "    }\n",
    "    try:\n",
    "        seasons_code = seasons_dict[season]\n",
    "    except KeyError:\n",
    "        raise\n",
    "    return seasons_code\n",
    "\n",
    "\n",
    "def md5_update_from_dir(directory, hash):\n",
    "    assert Path(directory).is_dir()\n",
    "    for path in sorted(Path(directory).iterdir()):\n",
    "        hash.update(path.name.encode())\n",
    "        if path.is_file():\n",
    "            with open(path, \"rb\") as f:\n",
    "                for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                    hash.update(chunk)\n",
    "        elif path.is_dir():\n",
    "            hash = md5_update_from_dir(path, hash)\n",
    "    return hash\n",
    "\n",
    "\n",
    "def md5_dir(directory):\n",
    "    return md5_update_from_dir(directory, md5()).hexdigest()\n",
    "\n",
    "\n",
    "def batch_process_playbills(root_dir, adminDB_collection, adminDB_next_item):\n",
    "    \n",
    "    directory_paths_list = sorted([x for x in root_dir.iterdir() if x.is_dir()])\n",
    "    \n",
    "    print(f'Processing {len(directory_paths_list)} directories')\n",
    "    \n",
    "    for directory_path in directory_paths_list:\n",
    "        \n",
    "        volume = Playbills(directory_path, adminDB_collection, adminDB_next_item)\n",
    "        print(volume.directory_path)\n",
    "        volume.process_publication()\n",
    "        \n",
    "        # increase adminDB_next_item number\n",
    "        adminDB_next_item += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ContinuingPublications_Volume Super Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     4,
     11,
     114,
     141,
     194,
     211
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# super class\n",
    "class ContinuingPublications_Volume:\n",
    "    '''Common base class for Continuing Publications'''\n",
    "\n",
    "    def __init__(self, directory):\n",
    "        self.directory_path = Path(directory).resolve()\n",
    "        self.input_directory_name = self.directory_path.name\n",
    "        # set yaml path, rows, and rows list\n",
    "        self.yaml_path = self.directory_path.parents[0].joinpath(f'{self.directory_path.name}.yml')  # yaml lives next to directory\n",
    "        \n",
    "        \n",
    "    def create_yaml(self):\n",
    "        \n",
    "        if self.yaml_path.is_file():\n",
    "            print(f'{self.yaml_path} already exists')\n",
    "            raise FileExistsError\n",
    "            return\n",
    "        else:  # create it\n",
    "            print(f'Creating {self.yaml_path}')\n",
    "            with open(self.yaml_path, 'a+') as yml_file:\n",
    "                for yaml_row in self.yaml_rows_list:\n",
    "                    yml_file.write(f'{yaml_row}\\n')  # add line break\n",
    "            # !touch \"{self.yaml_path}\"\n",
    "            # for yaml_row in self.yaml_rows_list:\n",
    "            #     print(f'Adding {yaml_row}')\n",
    "            #     !echo \"{yaml_row}\" >> \"{self.yaml_path}\"\n",
    "            # print(f'YAML data in {self.yaml_path}')\n",
    "            !cat \"{self.yaml_path}\"\n",
    "            return\n",
    "        \n",
    "        \n",
    "    def process_publication(self):\n",
    "        \n",
    "        # rename files\n",
    "        self.rename_files_to_directory_name('.tiff')  # process .tiff first just in case\n",
    "        self.rename_files_to_directory_name('.tif')  # then make sure it's all .tif\n",
    "        \n",
    "        # create YAML file, ingest directory\n",
    "        self.create_yaml()\n",
    "        ingest_directory_path = self.create_islandora_ingest_directory()\n",
    "        \n",
    "        # create Islandora-required book directory then move ingest directory into it\n",
    "        self.book_directory_path = self.directory_path.parents[0].joinpath('book')\n",
    "        self.book_directory_path.mkdir(exist_ok=True)\n",
    "        self.final_path = self.book_directory_path.joinpath(ingest_directory_path.name)\n",
    "        ingest_directory_path.replace(self.final_path)\n",
    "        \n",
    "        # move YAML file into book directory\n",
    "        new_yaml_path = self.book_directory_path.joinpath(self.yaml_path.name)\n",
    "        self.yaml_path.replace(new_yaml_path) \n",
    "\n",
    "        number_of_books = len([x for x in self.book_directory_path.iterdir() if x.is_dir()])\n",
    "        print(f'{number_of_books} books in {self.book_directory_path} for ingest')\n",
    "        print('')\n",
    "\n",
    "        \n",
    "    def backup_volume(self):\n",
    "        '''\n",
    "        -- Purpose --\n",
    "        Copy all files in directory to backup directory with name: <directory>_backup\n",
    "\n",
    "        -- Arguments --\n",
    "        None\n",
    "\n",
    "        -- Returns --\n",
    "        backup_directory_path: type=Path-like object; returns absolute path to backup directory\n",
    "        '''\n",
    "        backup_directory_path = self.directory_path.parent.joinpath(f'{self.directory_path.name}_backup')\n",
    "\n",
    "        if backup_directory_path.exists():  # copytree requires directory to NOT exist\n",
    "            # rmtree(backup_directory_path)\n",
    "            print(f'Backup already exists at {backup_directory_path}')\n",
    "        else:\n",
    "            print(f'Backing up {self.directory_path.name} . . .')\n",
    "            backup_directory_path.mkdir()\n",
    "            \n",
    "            # roll my own copytree since I'm having issues with permissions\n",
    "            everything_paths_list = list(self.directory_path.glob('**/*'))\n",
    "            \n",
    "            dirs_list = [x for x in everything_paths_list if x.is_dir()]\n",
    "            files_list = [x for x in everything_paths_list if x.is_file()]\n",
    "            \n",
    "            for dir_path in dirs_list:\n",
    "                # get dir_path without volume.directory_path at the beginning\n",
    "                local_path = str(dir_path).replace(str(self.directory_path), '')\n",
    "                # get a list of the directories by stripping the forward-slashes\n",
    "                local_path = local_path.strip('/')\n",
    "                # create output_dir and create it\n",
    "                output_dir = backup_directory_path.joinpath(local_path)\n",
    "                output_dir.mkdir(parents=True)\n",
    "                \n",
    "            for file_path in files_list:\n",
    "                # get local path for file (everything after volume.directory_path) as a list\n",
    "                local_path = str(file_path).replace(str(self.directory_path), '').strip('/')\n",
    "                # set output_path and copy\n",
    "                output_path = backup_directory_path.joinpath(local_path)\n",
    "                copy2(file_path, output_path)\n",
    "\n",
    "            # copytree(self.directory_path, backup_directory_path)\n",
    "\n",
    "            if backup_directory_path.exists():\n",
    "                self.input_hash = md5_dir(self.directory_path)\n",
    "                self.backup_hash = md5_dir(backup_directory_path)\n",
    "                \n",
    "                # check md5 hashes of backup against original\n",
    "                if self.input_hash != self.backup_hash:\n",
    "                    print(f'input hash: {self.input_hash}')\n",
    "                    print(f'backup hash: {self.backup_hash}')\n",
    "                    raise ValueError\n",
    "                    return\n",
    "                print('Backup created')\n",
    "        return backup_directory_path.resolve()\n",
    "\n",
    "    \n",
    "    def rename_PDFs_for_ingest(self):\n",
    "\n",
    "        pdf_paths_list = self.get_file_paths('.pdf')\n",
    "\n",
    "        number_of_pdfs = len(pdf_paths_list)\n",
    "        if number_of_pdfs == 0:\n",
    "            print(f'{number_of_pdfs} PDFs to process')\n",
    "        else:  # process PDFs\n",
    "            for pdf_path in pdf_paths_list:\n",
    "                # expect PDF stems ending in original or processed\n",
    "                if pdf_path.stem.lower().endswith('original'):\n",
    "                    new_pdf_path = pdf_path.parents[0].joinpath('ORIGINAL.pdf')\n",
    "                elif pdf_path.stem.lower().endswith('edited'):\n",
    "                    new_pdf_path = pdf_path.parents[0].joinpath('ORIGINAL_EDITED.pdf')\n",
    "                else:  # don't rename\n",
    "                    print(f'{pdf_path} is not original or original_edited, manually remediate')\n",
    "                    print('')\n",
    "                    continue\n",
    "                # rename PDF\n",
    "                print(f'Renaming {pdf_path.name} to {new_pdf_path}')\n",
    "                print('')\n",
    "                pdf_path.replace(new_pdf_path)\n",
    "\n",
    "        self.pdf_paths_list = self.get_file_paths('.pdf')\n",
    "        return self.pdf_paths_list\n",
    "\n",
    "    \n",
    "    def create_islandora_ingest_directory(self):\n",
    "        '''\n",
    "        -- Purpose --\n",
    "        Create Islandora ingest directory with TIFF in nested structure\n",
    "\n",
    "        -- Arguments --\n",
    "        None\n",
    "\n",
    "        -- Returns --\n",
    "        ingest_directory_path: type=Path-like object; Path to the directory for ingest\n",
    "        '''\n",
    "\n",
    "        # create ingest directory\n",
    "        ingest_directory_name = self.directory_path.name\n",
    "        ingest_directory_path = self.directory_path.parents[0].joinpath(ingest_directory_name)\n",
    "        # try:\n",
    "        #     ingest_directory_path.mkdir()\n",
    "        # except FileExistsError:  # directory already exists\n",
    "        #     print(f'WARNING: ingest directory already exists at {ingest_directory_path}')\n",
    "\n",
    "        self.directory_path.replace(ingest_directory_path)\n",
    "\n",
    "        image_paths_list = sorted([x for x in ingest_directory_path.glob('*.tif')])\n",
    "        number_of_images = len(image_paths_list)\n",
    "\n",
    "        print(f'Processing {number_of_images} images in {self.directory_path.name}')\n",
    "\n",
    "        # for each image\n",
    "        for index, image_path in enumerate(image_paths_list, start=1):\n",
    "\n",
    "            # create a sub-directory with a simple index number\n",
    "            image_subdirectory_path = ingest_directory_path.joinpath(str(index).zfill(6))\n",
    "            try:\n",
    "                image_subdirectory_path.mkdir()\n",
    "            except FileExistsError:\n",
    "                print(f'Sub-directory already exists at {image_subdirectory_path}')\n",
    "            \n",
    "            # if last 4 chars of subdirectory name != last 4 of image_path.stem\n",
    "            if image_subdirectory_path.name[-4:] != image_path.stem[-4:]:\n",
    "                raise ValueError\n",
    "\n",
    "            # set new image name and copy path, then copy image\n",
    "            #copy_image_path = image_subdirectory_path.joinpath(image_path.name)\n",
    "            #copyfile(image_path, copy_image_path)\n",
    "            image_path.replace(image_subdirectory_path.joinpath(image_path.name))\n",
    "\n",
    "        self.rename_PDFs_for_ingest()\n",
    "        print(f'Ingest directory created at {ingest_directory_path}')\n",
    "        print('')\n",
    "\n",
    "        return ingest_directory_path\n",
    "\n",
    "    \n",
    "    def get_file_paths(self, with_extension):\n",
    "        '''\n",
    "        -- Purpose --\n",
    "        Get all file Paths with_extension in self.directory_path\n",
    "\n",
    "        -- Arguments --\n",
    "        with_extension: type=string; extension to use for globbing\n",
    "\n",
    "        -- Returns --\n",
    "        file_paths_list: type:list; list of Path-like objects, 1 Path-like object\n",
    "        per file_path in self.directory_path\n",
    "        '''\n",
    "        formatted_extension = get_formatted_extension(with_extension)\n",
    "        file_paths_list = sorted(self.directory_path.glob(f'*{formatted_extension}'))\n",
    "        return file_paths_list\n",
    "\n",
    "    \n",
    "    def rename_files_to_directory_name(self, with_extension, zerofill=4, debug=False):\n",
    "        '''\n",
    "        -- Purpose --\n",
    "        Rename all files {with_extension} to {self.directory_path.name}_{str(index).zfill(zerofill)}\n",
    "        *Note: will currently remediate extensions to lower-case and change tiff/jpeg to tif/jpg\n",
    "\n",
    "        -- Arguments --\n",
    "        with_extension: type=string; extension to rename\n",
    "        zerofill: type=integer; how many digits to zeropad\n",
    "\n",
    "        -- Returns --\n",
    "        None\n",
    "        '''\n",
    "        formatted_extension = get_formatted_extension(with_extension)\n",
    "\n",
    "        # extension will be lower-case and tif/jpg instead of tiff/jpeg\n",
    "        remediated_extension = get_formatted_extension(with_extension, remediate=True)\n",
    "\n",
    "        # get total number of files and the paths for files to rename\n",
    "        file_paths_list = self.get_file_paths(formatted_extension)\n",
    "        number_of_files = len(file_paths_list)\n",
    "\n",
    "        print(f'{number_of_files} with {formatted_extension}')\n",
    "\n",
    "        if number_of_files == 0:\n",
    "            print('0 files to process')\n",
    "            pass\n",
    "\n",
    "        else:  # rename files\n",
    "            backup_directory_path = self.backup_volume()\n",
    "\n",
    "            print(f'Renaming {number_of_files} \"{formatted_extension}\"s in {self.directory_path.name} . . .')\n",
    "\n",
    "            count = 0\n",
    "            try:\n",
    "                for index, file_path in enumerate(file_paths_list, start=1):\n",
    "                    # rename TIFF files from Adobe Acrobat for Islandora ingest, i.e. FILENAME.extension\n",
    "                    new_file_name = f'{self.directory_path.name.upper()}_{str(index).zfill(zerofill)}{remediated_extension}'\n",
    "                    new_file_path = file_path.parents[0].joinpath(new_file_name)\n",
    "                    file_path.replace(new_file_path)\n",
    "                    if debug:\n",
    "                        print(f'old: {file_path.name}')\n",
    "                        print(f'new: {new_file_path.name}')\n",
    "                    count = index\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "            print(f' Renamed {count} \"{formatted_extension}\"s')\n",
    "            print('') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playbills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playbill YAML files\n",
    "\n",
    "Name directories with *YYYY-MM-DD_Title_of_Playbill*\n",
    "\n",
    "https://github.com/utkdigitalinitiatives/Automated-Ingest-for-Continuing-Publications/blob/master/collection_templates/collections__playbills.yml\n",
    "\n",
    "1. adminDB: \"0012_003049_XXXXXX\" # Replace \"X\" with appropriate adminDB values.\n",
    "1. title: \"Title\" # Replace with title in title case.\n",
    "1. date_issued: \"Month Day, YYYY\" # Replace with date values (e.g. September 21, 2019)\n",
    "1. date_issued_edtf: \"YYYY-MM-DD\" # Replace letters with a four-digit year and a two-digit number for month and day (e.g. 2019-09-21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playbills Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Playbills(ContinuingPublications_Volume):\n",
    "    def __init__(self, directory, adminDB_collection, adminDB_item):\n",
    "        # load ContinuingPublications_Volume class\n",
    "        super().__init__(directory)\n",
    "        \n",
    "        # get metadata from filename, ex: 2018-10-04_It's_a_Wonderful_Life\n",
    "        self.date, self.title = self.directory_path.name.split('_', maxsplit=1)\n",
    "        self.title_replace_underscores = self.title.replace('_', ' ')\n",
    "        self.yyyy, self.mm, self.dd = self.date.split('-')\n",
    "        self.parsed_date = parse(self.date)\n",
    "        self.month = self.parsed_date.strftime(\"%B\")\n",
    "        \n",
    "        # cast self.dd as int to remove a possible leading zero\n",
    "        self.date_issued = f'{self.month} {int(self.dd)}, {self.yyyy}'\n",
    "        self.date_issued_edtf = self.date\n",
    "        self.adminDB = f'0012_{str(adminDB_collection).zfill(6)}_{str(adminDB_item).zfill(6)}'\n",
    "        self.yaml_row_0 = f'''adminDB: \"{self.adminDB}\"'''\n",
    "        self.yaml_row_1 = f'''title: \"{self.title_replace_underscores}\"'''\n",
    "        self.yaml_row_2 = f'''date_issued: \"{self.date_issued}\"'''\n",
    "        self.yaml_row_3 = f'''date_issued_edtf: \"{self.date_issued_edtf}\"'''\n",
    "        self.yaml_rows_list = [self.yaml_row_0, self.yaml_row_1, self.yaml_row_2, self.yaml_row_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Playbills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory_path = Path('/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills')\n",
    "adminDB_collection = 3049\n",
    "adminDB_next_item = 875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_process_playbills(root_directory_path, adminDB_collection, adminDB_next_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phoenix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phoenix YAML files\n",
    "\n",
    "Name directories: *phoenix_YYYY-season*\n",
    "\n",
    "https://github.com/utkdigitalinitiatives/Automated-Ingest-for-Continuing-Publications/blob/master/collection_templates/collections__phoenix.yml\n",
    "\n",
    "1. year: \"YYYY\" # Replace \"X\" a four-digit year\n",
    "1. season: \"season\" # Replace season with 'spring', 'summer', 'fall', or 'winter' (note lowercase) as appropriate\n",
    "1. date_issued_edtf: \"YYYY-SS\" # Replace letters with a four-digit year and a two-digit number for season, spring = '21', summer= '22', fall= '23', winter= '24'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phoenix Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Phoenix(ContinuingPublications_Volume):\n",
    "    def __init__(self, directory):\n",
    "        # load ContinuingPublications_Volume class\n",
    "        super().__init__(directory)\n",
    "        \n",
    "        # get metadata from filename, ex: phoenix_2018-winter\n",
    "        self.title, self.date = self.directory_path.name.split('_', maxsplit=1)\n",
    "        self.yyyy, self.season = self.date.split('-')\n",
    "        # seasons should be lowercase\n",
    "        self.season = self.season.lower()\n",
    "        # get season code\n",
    "        self.season_code = get_season_code(self.season)  # season code is string\n",
    "        self.date_issued_edtf = f'{self.yyyy}-{self.season_code}'\n",
    "        self.yaml_row_0 = f'''year: \"{self.yyyy}\"'''\n",
    "        self.yaml_row_1 = f'''season: \"{self.season}\"'''\n",
    "        self.yaml_row_2 = f'''date_issued_edtf: \"{self.date_issued_edtf}\"'''\n",
    "        self.yaml_rows_list = [self.yaml_row_0, self.yaml_row_1, self.yaml_row_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoenix_root_path = Path('/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Phoenix')\n",
    "phoenix_dir_paths_list = sorted([x for x in phoenix_root_path.iterdir() if x.is_dir()])\n",
    "phoenix_dir_paths_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for phoenix_dir_path in phoenix_dir_paths_list:\n",
    "        \n",
    "        volume = Phoenix(phoenix_dir_path)\n",
    "        print(volume.directory_path)\n",
    "        volume.process_publication()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchbearer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchbearer YAML files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name directories: *torchbearer_VV-N_YYYY-season*\n",
    "\n",
    "https://github.com/utkdigitalinitiatives/Automated-Ingest-for-Continuing-Publications/blob/master/collection_templates/collections__torch.yml\n",
    "\n",
    "1. volume: \"XX\" # Replace XX with the volume number\n",
    "1. number: \"X\" # Replace X with the number or issue number\n",
    "1. date_issued: \"Season YYYY\" # Replace with the season fully written out and the four-digit year, e.g. Spring 2019\n",
    "1. date_issued_edtf: \"YYYY-MM\" # Replace letters with a four-digit year and a two-digit number for season (spring = 21, summer = 22, fall = 23, winter = 24), e.g. 2019-21 for Spring 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torcherbearer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Torchbearer(ContinuingPublications_Volume):\n",
    "    def __init__(self, directory):\n",
    "        # load ContinuingPublications_Volume class\n",
    "        super().__init__(directory)\n",
    "        \n",
    "        # get metadata from filename, ex: torchbearer_v53-n2_2018-fall\n",
    "        self.title, self.volume_info, self.date = self.directory_path.name.split('_')\n",
    "        self.volume, self.number = self.volume_info.split('-')\n",
    "        self.yyyy, self.season = self.date.split('-')\n",
    "        # seasons should be lowercase\n",
    "        self.season = self.season.lower()\n",
    "        # get season code\n",
    "        self.season_code = get_season_code(self.season)  # season code is string\n",
    "        self.date_issued = f'{self.season.capitalize()} {self.yyyy}'\n",
    "        self.date_issued_edtf = f'{self.yyyy}-{self.season_code}'\n",
    "        self.yaml_row_0 = f'''volume: \"{self.volume}\"'''\n",
    "        self.yaml_row_1 = f'''number: \"{self.number}\"'''\n",
    "        self.yaml_row_2 = f'''date_issued: \"{self.date_issued}\"'''\n",
    "        self.yaml_row_3 = f'''date_issued_edtf: \"{self.date_issued_edtf}\"'''\n",
    "        self.yaml_rows_list = [self.yaml_row_0, self.yaml_row_1, self.yaml_row_2, self.yaml_row_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Torchbearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchbearer_root_path = Path('/Users/jeremy/Documents/GitHub/utk_ContinuingPublications/data')\n",
    "torchbearer_dir_paths_list = sorted([x for x in torchbearer_root_path.iterdir() if x.is_dir()])\n",
    "torchbearer_dir_paths_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for torchbearer_dir_path in torchbearer_dir_paths_list:\n",
    "        \n",
    "        volume = Torchbearer(torchbearer_dir_path)\n",
    "        print(volume.directory_path)\n",
    "        volume.process_publication()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoky Mountain Hiking Club Handbooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMHC Handbooks YAML files\n",
    "\n",
    "Name directories: *smhc-handbook_YYYY*\n",
    "\n",
    "https://github.com/utkdigitalinitiatives/Automated-Ingest-for-Continuing-Publications/blob/master/collection_templates/gsmrc__smhc.yml\n",
    "\n",
    "1. title: \"YYYY Handbook of the Smoky Mountains Hiking Club\" # Replace 'YYYY' with four-digit year\n",
    "1. year: \"YYYY\" # Replace 'YYYY' with four-digit year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMHC Handbooks Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SMHC_Handbook(ContinuingPublications_Volume):\n",
    "    def __init__(self, directory):\n",
    "        # load ContinuingPublications_Volume class\n",
    "        super().__init__(directory)\n",
    "        \n",
    "        # get metadata from filename, ex: torchbearer_v53-n2_2018-fall\n",
    "        self.title, self.yyyy = self.directory_path.name.split('_')\n",
    "        self.yaml_row_0 = f'''title: \"{self.yyyy} Handbook of the Smoky Mountains Hiking Club\"'''\n",
    "        self.yaml_row_1 = f'''year: \"{self.yyyy}\"'''\n",
    "        self.yaml_rows_list = [self.yaml_row_0, self.yaml_row_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process SMHC Handbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/SmokyMountainsHikingClub_handbooks/smhc-handbook_2018'),\n",
       " PosixPath('/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/SmokyMountainsHikingClub_handbooks/smhc-handbook_2019')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smhc_root_path = Path('/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/SmokyMountainsHikingClub_handbooks')\n",
    "smhc_dir_paths_list = sorted([x for x in smhc_root_path.iterdir() if x.is_dir()])\n",
    "smhc_dir_paths_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/SmokyMountainsHikingClub_handbooks/smhc-handbook_2018\n",
      "0 with .tiff\n",
      "0 files to process\n",
      "84 with .tif\n",
      "Backing up smhc-handbook_2018 . . .\n",
      "Backup created\n",
      "Renaming 84 \".tif\"s in smhc-handbook_2018 . . .\n",
      " Renamed 84 \".tif\"s\n",
      "\n",
      "Creating /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/SmokyMountainsHikingClub_handbooks/smhc-handbook_2018.yml\n",
      "title: \"2018 Handbook of the Smoky Mountains Hiking Club\"\n",
      "year: \"2018\"\n",
      "Processing 84 images in smhc-handbook_2018\n",
      "0 PDFs to process\n",
      "Ingest directory created at /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/SmokyMountainsHikingClub_handbooks/smhc-handbook_2018\n",
      "\n",
      "1 books in /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/SmokyMountainsHikingClub_handbooks/book for ingest\n",
      "\n",
      "/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/SmokyMountainsHikingClub_handbooks/smhc-handbook_2019\n",
      "0 with .tiff\n",
      "0 files to process\n",
      "80 with .tif\n",
      "Backing up smhc-handbook_2019 . . .\n",
      "Backup created\n",
      "Renaming 80 \".tif\"s in smhc-handbook_2019 . . .\n",
      " Renamed 80 \".tif\"s\n",
      "\n",
      "Creating /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/SmokyMountainsHikingClub_handbooks/smhc-handbook_2019.yml\n",
      "title: \"2019 Handbook of the Smoky Mountains Hiking Club\"\n",
      "year: \"2019\"\n",
      "Processing 80 images in smhc-handbook_2019\n",
      "0 PDFs to process\n",
      "Ingest directory created at /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/SmokyMountainsHikingClub_handbooks/smhc-handbook_2019\n",
      "\n",
      "2 books in /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/SmokyMountainsHikingClub_handbooks/book for ingest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for smhc_dir_path in smhc_dir_paths_list:\n",
    "        \n",
    "        volume = SMHC_Handbook(smhc_dir_path)\n",
    "        print(volume.directory_path)\n",
    "        volume.process_publication()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commencements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commencements YAML files\n",
    "\n",
    "Name directories: *commencement_YYYY-season*\n",
    "\n",
    "https://github.com/utkdigitalinitiatives/Automated-Ingest-for-Continuing-Publications/blob/master/collection_templates/collections__commencements.yml\n",
    "\n",
    "1. year: \"YYYY\" # Replace \"X\" a four-digit year\n",
    "1. season: \"season\" # Replace season with 'spring', 'summer', 'fall', or 'winter' (note lowercase) as appropriate\n",
    "1. date_created_edtf: \"YYYY-SS\" # Replace letters with a four-digit year and a two-digit number for season, spring = '21', summer= '22', fall= '23', winter= '24'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commencements Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Commencements(ContinuingPublications_Volume):\n",
    "    def __init__(self, directory):\n",
    "        # load ContinuingPublications_Volume class\n",
    "        super().__init__(directory)\n",
    "        \n",
    "        # get metadata from filename, ex: torchbearer_v53-n2_2018-fall\n",
    "        self.title, self.date = self.directory_path.name.split('_')\n",
    "        self.yyyy, self.season = self.date.split('-')\n",
    "        # seasons should be lowercase\n",
    "        self.season = self.season.lower()\n",
    "        # get season code\n",
    "        self.season_code = get_season_code(self.season)  # season code is string\n",
    "        self.date_created_edtf = f'{self.yyyy}-{self.season_code}'\n",
    "        self.yaml_row_0 = f'''year: \"{self.yyyy}\"'''\n",
    "        self.yaml_row_1 = f'''season: \"{self.season}\"'''\n",
    "        self.yaml_row_2 = f'''date_created_edtf: \"{self.date_created_edtf}\"'''\n",
    "        self.yaml_rows_list = [self.yaml_row_0, self.yaml_row_1, self.yaml_row_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Commencements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commencements_root_path = Path('/Users/jeremy/Documents/GitHub/utk_ContinuingPublications/data')\n",
    "commencements_dir_paths_list = sorted([x for x in commencements_root_path.iterdir() if x.is_dir()])\n",
    "commencements_dir_paths_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "for commencements_dir_path in commencements_dir_paths_list:\n",
    "        \n",
    "        volume = Commencements(commencements_dir_path)\n",
    "        print(volume.directory_path)\n",
    "        volume.process_publication()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alumnus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alumnus YAML files\n",
    "\n",
    "Name directories: *alumnus_YYYY-season*\n",
    "\n",
    "https://github.com/utkdigitalinitiatives/Automated-Ingest-for-Continuing-Publications/blob/master/collection_templates/collections__alumnus.yml\n",
    "\n",
    "1. year: \"YYYY\" # Replace \"X\" a four-digit year\n",
    "1. season: \"season\" # Replace season with 'spring', 'summer', 'fall', or 'winter' (note lowercase) as appropriate\n",
    "1. date_issued_edtf: \"YYYY-SS\" # Replace letters with a four-digit year and a two-digit number for season, spring = '21', summer= '22', fall= '23', winter= '24'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alumnus Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Alumnus(ContinuingPublications_Volume):\n",
    "    def __init__(self, directory):\n",
    "        # load ContinuingPublications_Volume class\n",
    "        super().__init__(directory)\n",
    "        \n",
    "        # get metadata from filename, ex: torchbearer_v53-n2_2018-fall\n",
    "        self.title, self.date = self.directory_path.name.split('_')\n",
    "        self.yyyy, self.season = self.date.split('-')\n",
    "        # seasons should be lowercase\n",
    "        self.season = self.season.lower()\n",
    "        # get season code\n",
    "        self.season_code = get_season_code(self.season)  # season code is string\n",
    "        self.date_issued_edtf = f'{self.yyyy}-{self.season_code}'\n",
    "        self.yaml_row_0 = f'''year: \"{self.yyyy}\"'''\n",
    "        self.yaml_row_1 = f'''season: \"{self.season}\"'''\n",
    "        self.yaml_row_2 = f'''date_issued_edtf: \"{self.date_issued_edtf}\"'''\n",
    "        self.yaml_rows_list = [self.yaml_row_0, self.yaml_row_1, self.yaml_row_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Alumnus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-fall'),\n",
       " PosixPath('/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-spring')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alumnus_root_path = Path('/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus')\n",
    "alumnus_dir_paths_list = sorted([x for x in alumnus_root_path.iterdir() if x.is_dir()])\n",
    "alumnus_dir_paths_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-fall\n",
      "0 with .tiff\n",
      "0 files to process\n",
      "68 with .tif\n",
      "Backing up alumnus_2017-fall . . .\n",
      "Backup created\n",
      "Renaming 68 \".tif\"s in alumnus_2017-fall . . .\n",
      " Renamed 68 \".tif\"s\n",
      "\n",
      "Creating /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-fall.yml\n",
      "year: \"2017\"\n",
      "season: \"fall\"\n",
      "date_issued_edtf: \"2017-23\"\n",
      "Processing 68 images in alumnus_2017-fall\n",
      "Renaming ._alum_2017fall_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-fall/ORIGINAL.pdf\n",
      "\n",
      "Renaming alum_2017fall_edited.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-fall/ORIGINAL_EDITED.pdf\n",
      "\n",
      "Renaming alum_2017fall_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-fall/ORIGINAL.pdf\n",
      "\n",
      "Ingest directory created at /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-fall\n",
      "\n",
      "1 books in /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/book for ingest\n",
      "\n",
      "/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-spring\n",
      "0 with .tiff\n",
      "0 files to process\n",
      "68 with .tif\n",
      "Backing up alumnus_2017-spring . . .\n",
      "Backup created\n",
      "Renaming 68 \".tif\"s in alumnus_2017-spring . . .\n",
      " Renamed 68 \".tif\"s\n",
      "\n",
      "Creating /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-spring.yml\n",
      "year: \"2017\"\n",
      "season: \"spring\"\n",
      "date_issued_edtf: \"2017-21\"\n",
      "Processing 68 images in alumnus_2017-spring\n",
      "Renaming alum_2017spring_edited.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-spring/ORIGINAL_EDITED.pdf\n",
      "\n",
      "Renaming alum_2017spring_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-spring/ORIGINAL.pdf\n",
      "\n",
      "Ingest directory created at /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-spring\n",
      "\n",
      "2 books in /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/book for ingest\n",
      "\n",
      "/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-winter\n",
      "0 with .tiff\n",
      "0 files to process\n",
      "68 with .tif\n",
      "Backing up alumnus_2017-winter . . .\n",
      "Backup created\n",
      "Renaming 68 \".tif\"s in alumnus_2017-winter . . .\n",
      " Renamed 68 \".tif\"s\n",
      "\n",
      "Creating /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-winter.yml\n",
      "year: \"2017\"\n",
      "season: \"winter\"\n",
      "date_issued_edtf: \"2017-24\"\n",
      "Processing 68 images in alumnus_2017-winter\n",
      "Renaming alum_2017winter_edited.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-winter/ORIGINAL_EDITED.pdf\n",
      "\n",
      "Renaming alum_2017winter_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-winter/ORIGINAL.pdf\n",
      "\n",
      "Ingest directory created at /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2017-winter\n",
      "\n",
      "3 books in /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/book for ingest\n",
      "\n",
      "/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2018-winter\n",
      "0 with .tiff\n",
      "0 files to process\n",
      "52 with .tif\n",
      "Backing up alumnus_2018-winter . . .\n",
      "Backup created\n",
      "Renaming 52 \".tif\"s in alumnus_2018-winter . . .\n",
      " Renamed 52 \".tif\"s\n",
      "\n",
      "Creating /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2018-winter.yml\n",
      "year: \"2018\"\n",
      "season: \"winter\"\n",
      "date_issued_edtf: \"2018-24\"\n",
      "Processing 52 images in alumnus_2018-winter\n",
      "Renaming ._alum_2018winter_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2018-winter/ORIGINAL.pdf\n",
      "\n",
      "Renaming alum_2018winter_edited.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2018-winter/ORIGINAL_EDITED.pdf\n",
      "\n",
      "Renaming alum_2018winter_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2018-winter/ORIGINAL.pdf\n",
      "\n",
      "Ingest directory created at /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/alumnus_2018-winter\n",
      "\n",
      "4 books in /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Alumnus/book for ingest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for alumnus_dir_path in alumnus_dir_paths_list:\n",
    "        \n",
    "        volume = Alumnus(alumnus_dir_path)\n",
    "        print(volume.directory_path)\n",
    "        volume.process_publication()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
