{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Continuing Publications: Playbills\n",
    "\n",
    "date: 2019-09-24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playbills YAML files\n",
    "\n",
    "https://github.com/utkdigitalinitiatives/Automated-Ingest-for-Continuing-Publications/blob/master/collection_templates/collections__playbills.yml\n",
    "\n",
    "1. adminDB: \"0012_003049_XXXXXX\" # Replace \"X\" with appropriate adminDB values.\n",
    "1. title: \"Title\" # Replace with title in title case.\n",
    "1. date_Issued: \"Month Day, YYYY\" # Replace with date values (e.g. September 21, 2019)\n",
    "1. date_Issued_edtf: \"YYYY-MM-DD\" # Replace letters with a four-digit year and a two-digit number for month and day (e.g. 2019-09-21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from hashlib import md5\n",
    "from pathlib import Path\n",
    "from shutil import copy2, rmtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def get_formatted_extension(from_extension, remediate=False):\n",
    "    '''\n",
    "    -- Purpose --\n",
    "    Returns an extension that:\n",
    "    1. has a period in the front\n",
    "    2. Optional: is lower-case\n",
    "    3. Optional: return jpeg as jpg and tiff as tif\n",
    "\n",
    "    -- Arguments --\n",
    "    from_extension: type=string; file extension with or without a '.'\n",
    "\n",
    "    -- Returns --\n",
    "    formatted_extension: type=string; formatted extension\n",
    "    '''\n",
    "    # make sure there's a period at the front of the extension\n",
    "    if from_extension.startswith('.'):  # do nothing\n",
    "        formatted_extension = from_extension\n",
    "    else:  # add a period\n",
    "        formatted_extension = f'.{from_extension}'\n",
    "\n",
    "    # make it lower-case\n",
    "    if remediate:\n",
    "        formatted_extension = formatted_extension.lower()\n",
    "        # hard-coded alterations for jpeg and tiff\n",
    "        if formatted_extension == '.jpeg':\n",
    "            formatted_extension = '.jpg'\n",
    "        elif formatted_extension == '.tiff':\n",
    "            formatted_extension = '.tif'\n",
    "\n",
    "    return formatted_extension\n",
    "\n",
    "\n",
    "def md5_update_from_dir(directory, hash):\n",
    "    assert Path(directory).is_dir()\n",
    "    for path in sorted(Path(directory).iterdir()):\n",
    "        hash.update(path.name.encode())\n",
    "        if path.is_file():\n",
    "            with open(path, \"rb\") as f:\n",
    "                for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                    hash.update(chunk)\n",
    "        elif path.is_dir():\n",
    "            hash = md5_update_from_dir(path, hash)\n",
    "    return hash\n",
    "\n",
    "\n",
    "def md5_dir(directory):\n",
    "    return md5_update_from_dir(directory, md5()).hexdigest()\n",
    "\n",
    "\n",
    "def batch_process_playbills(root_dir, adminDB_collection, adminDB_next_item):\n",
    "    \n",
    "    directory_paths_list = sorted([x for x in root_dir.iterdir() if x.is_dir()])\n",
    "    \n",
    "    print(f'Processing {len(directory_paths_list)} directories')\n",
    "    \n",
    "    for directory_path in directory_paths_list:\n",
    "        \n",
    "        volume = Playbills(directory_path, adminDB_collection, adminDB_next_item)\n",
    "        print(volume.directory_path)\n",
    "        volume.process_publication()\n",
    "        \n",
    "        # increase adminDB_next_item number\n",
    "        adminDB_next_item += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     67,
     146,
     163
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class ContinuingPublications_Volume:\n",
    "    '''Common base class for Continuing Publications'''\n",
    "\n",
    "    def __init__(self, directory, adminDB_collection, adminDB_item):\n",
    "        self.directory_path = Path(directory).resolve()\n",
    "        self.input_directory_name = self.directory_path.name\n",
    "        # set yaml path, rows, and rows list\n",
    "        self.yaml_path = self.directory_path.parents[0].joinpath(f'{self.directory_path.name}.yml')  # yaml lives next to directory\n",
    "\n",
    "    def backup_volume(self):\n",
    "        '''\n",
    "        -- Purpose --\n",
    "        Copy all files in directory to backup directory with name: <directory>_backup\n",
    "\n",
    "        -- Arguments --\n",
    "        None\n",
    "\n",
    "        -- Returns --\n",
    "        backup_directory_path: type=Path-like object; returns absolute path to backup directory\n",
    "        '''\n",
    "        backup_directory_path = self.directory_path.parent.joinpath(f'{self.directory_path.name}_backup')\n",
    "\n",
    "        if backup_directory_path.exists():  # copytree requires directory to NOT exist\n",
    "            # rmtree(backup_directory_path)\n",
    "            print(f'Backup already exists at {backup_directory_path}')\n",
    "        else:\n",
    "            print(f'Backing up {self.directory_path.name} . . .')\n",
    "            backup_directory_path.mkdir()\n",
    "            \n",
    "            # roll my own copytree since I'm having issues with permissions\n",
    "            everything_paths_list = list(self.directory_path.glob('**/*'))\n",
    "            \n",
    "            dirs_list = [x for x in everything_paths_list if x.is_dir()]\n",
    "            files_list = [x for x in everything_paths_list if x.is_file()]\n",
    "            \n",
    "            for dir_path in dirs_list:\n",
    "                # get dir_path without volume.directory_path at the beginning\n",
    "                local_path = str(dir_path).replace(str(self.directory_path), '')\n",
    "                # get a list of the directories by stripping the forward-slashes\n",
    "                local_path = local_path.strip('/')\n",
    "                # create output_dir and create it\n",
    "                output_dir = backup_directory_path.joinpath(local_path)\n",
    "                output_dir.mkdir(parents=True)\n",
    "                \n",
    "            for file_path in files_list:\n",
    "                # get local path for file (everything after volume.directory_path) as a list\n",
    "                local_path = str(file_path).replace(str(self.directory_path), '').strip('/')\n",
    "                # set output_path and copy\n",
    "                output_path = backup_directory_path.joinpath(local_path)\n",
    "                copy2(file_path, output_path)\n",
    "\n",
    "            # copytree(self.directory_path, backup_directory_path)\n",
    "\n",
    "            if backup_directory_path.exists():\n",
    "                self.input_hash = md5_dir(self.directory_path)\n",
    "                self.backup_hash = md5_dir(backup_directory_path)\n",
    "                \n",
    "                # check md5 hashes of backup against original\n",
    "                if self.input_hash != self.backup_hash:\n",
    "                    print(f'input hash: {self.input_hash}')\n",
    "                    print(f'backup hash: {self.backup_hash}')\n",
    "                    raise ValueError\n",
    "                    return\n",
    "                print('Backup created')\n",
    "        return backup_directory_path.resolve()\n",
    "\n",
    "    \n",
    "    def rename_PDFs_for_ingest(self):\n",
    "\n",
    "        pdf_paths_list = self.get_file_paths('.pdf')\n",
    "\n",
    "        number_of_pdfs = len(pdf_paths_list)\n",
    "        if number_of_pdfs == 0:\n",
    "            print(f'{number_of_pdfs} PDFs to process')\n",
    "        else:  # process PDFs\n",
    "            for pdf_path in pdf_paths_list:\n",
    "                # expect PDF stems ending in original or processed\n",
    "                if pdf_path.stem.lower().endswith('original'):\n",
    "                    new_pdf_path = pdf_path.parents[0].joinpath('ORIGINAL.pdf')\n",
    "                elif pdf_path.stem.lower().endswith('edited'):\n",
    "                    new_pdf_path = pdf_path.parents[0].joinpath('ORIGINAL_EDITED.pdf')\n",
    "                else:  # don't rename\n",
    "                    print(f'{pdf_path} is not original or original_edited, manually remediate')\n",
    "                    print('')\n",
    "                    continue\n",
    "                # rename PDF\n",
    "                print(f'Renaming {pdf_path.name} to {new_pdf_path}')\n",
    "                print('')\n",
    "                pdf_path.replace(new_pdf_path)\n",
    "\n",
    "        self.pdf_paths_list = self.get_file_paths('.pdf')\n",
    "        return self.pdf_paths_list\n",
    "\n",
    "    \n",
    "    def create_islandora_ingest_directory(self):\n",
    "        '''\n",
    "        -- Purpose --\n",
    "        Create Islandora ingest directory with TIFF in nested structure\n",
    "\n",
    "        -- Arguments --\n",
    "        None\n",
    "\n",
    "        -- Returns --\n",
    "        ingest_directory_path: type=Path-like object; Path to the directory for ingest\n",
    "        '''\n",
    "\n",
    "        # create ingest directory\n",
    "        ingest_directory_name = self.directory_path.name\n",
    "        ingest_directory_path = self.directory_path.parents[0].joinpath(ingest_directory_name)\n",
    "        # try:\n",
    "        #     ingest_directory_path.mkdir()\n",
    "        # except FileExistsError:  # directory already exists\n",
    "        #     print(f'WARNING: ingest directory already exists at {ingest_directory_path}')\n",
    "\n",
    "        self.directory_path.replace(ingest_directory_path)\n",
    "\n",
    "        image_paths_list = [x for x in ingest_directory_path.glob('*.tif')]\n",
    "        number_of_images = len(image_paths_list)\n",
    "\n",
    "        print(f'Processing {number_of_images} images in {self.directory_path.name}')\n",
    "\n",
    "        # for each image\n",
    "        for index, image_path in enumerate(image_paths_list, start=1):\n",
    "\n",
    "            # create a sub-directory with a simple index number\n",
    "            image_subdirectory_path = ingest_directory_path.joinpath(str(index).zfill(6))\n",
    "            try:\n",
    "                image_subdirectory_path.mkdir()\n",
    "            except FileExistsError:\n",
    "                print(f'Sub-directory already exists at {image_subdirectory_path}')\n",
    "\n",
    "            # set new image name and copy path, then copy image\n",
    "            #copy_image_path = image_subdirectory_path.joinpath(image_path.name)\n",
    "            #copyfile(image_path, copy_image_path)\n",
    "            image_path.replace(image_subdirectory_path.joinpath(image_path.name))\n",
    "\n",
    "        self.rename_PDFs_for_ingest()\n",
    "        print(f'Ingest directory created at {ingest_directory_path}')\n",
    "        print('')\n",
    "\n",
    "        return ingest_directory_path\n",
    "\n",
    "    \n",
    "    def get_file_paths(self, with_extension):\n",
    "        '''\n",
    "        -- Purpose --\n",
    "        Get all file Paths with_extension in self.directory_path\n",
    "\n",
    "        -- Arguments --\n",
    "        with_extension: type=string; extension to use for globbing\n",
    "\n",
    "        -- Returns --\n",
    "        file_paths_list: type:list; list of Path-like objects, 1 Path-like object\n",
    "        per file_path in self.directory_path\n",
    "        '''\n",
    "        formatted_extension = get_formatted_extension(with_extension)\n",
    "        file_paths_list = sorted(self.directory_path.glob(f'*{formatted_extension}'))\n",
    "        return file_paths_list\n",
    "\n",
    "    \n",
    "    def rename_files_to_directory_name(self, with_extension, zerofill=4):\n",
    "        '''\n",
    "        -- Purpose --\n",
    "        Rename all files {with_extension} to {self.directory_path.name}_{str(index).zfill(zerofill)}\n",
    "        *Note: will currently remediate extensions to lower-case and change tiff/jpeg to tif/jpg\n",
    "\n",
    "        -- Arguments --\n",
    "        with_extension: type=string; extension to rename\n",
    "        zerofill: type=integer; how many digits to zeropad\n",
    "\n",
    "        -- Returns --\n",
    "        None\n",
    "        '''\n",
    "        formatted_extension = get_formatted_extension(with_extension)\n",
    "\n",
    "        # extension will be lower-case and tif/jpg instead of tiff/jpeg\n",
    "        remediated_extension = get_formatted_extension(with_extension, remediate=True)\n",
    "\n",
    "        # get total number of files and the paths for files to rename\n",
    "        file_paths_list = self.get_file_paths(formatted_extension)\n",
    "        number_of_files = len(file_paths_list)\n",
    "\n",
    "        print(f'{number_of_files} with {formatted_extension}')\n",
    "\n",
    "        if number_of_files == 0:\n",
    "            print('0 files to process')\n",
    "            pass\n",
    "\n",
    "        else:  # rename files\n",
    "            backup_directory_path = self.backup_volume()\n",
    "\n",
    "            print(f'Renaming {number_of_files} \"{formatted_extension}\"s in {self.directory_path.name} . . .')\n",
    "\n",
    "            count = 0\n",
    "            try:\n",
    "                for index, file_path in enumerate(file_paths_list, start=1):\n",
    "                    # rename TIFF files from Adobe Acrobat for Islandora ingest, i.e. FILENAME.extension\n",
    "                    new_file_name = f'{self.directory_path.name.upper()}_{str(index).zfill(zerofill)}{remediated_extension}'\n",
    "                    new_file_path = file_path.parents[0].joinpath(new_file_name)\n",
    "                    file_path.replace(new_file_path)\n",
    "                    count = index\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "            print(f' Renamed {count} \"{formatted_extension}\"s')\n",
    "            print('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Playbills(ContinuingPublications_Volume):\n",
    "    def __init__(self, directory, adminDB_collection, adminDB_item):\n",
    "        # load ContinuingPublications_Volume class\n",
    "        super().__init__(directory, adminDB_collection, adminDB_item)\n",
    "        \n",
    "        # get metadata from filename\n",
    "        self.date, self.title = self.directory_path.name.split('_', maxsplit=1)\n",
    "        self.title_replace_underscores = self.title.replace('_', ' ')\n",
    "        self.yyyy, self.mm, self.dd = self.date.split('-')\n",
    "        self.parsed_date = parse(self.date)\n",
    "        self.month = self.parsed_date.strftime(\"%B\")\n",
    "        \n",
    "        # cast self.dd as int to remove a possible leading zero\n",
    "        self.date_issued = f'{self.month} {int(self.dd)}, {self.yyyy}'\n",
    "        self.date_issued_edtf = self.date\n",
    "        self.adminDB = f'0012_{str(adminDB_collection).zfill(6)}_{str(adminDB_item).zfill(6)}'\n",
    "        self.yaml_row_0 = f'''adminDB: \"{self.adminDB}\"'''\n",
    "        self.yaml_row_1 = f'''Title: \"{self.title_replace_underscores}\"'''\n",
    "        self.yaml_row_2 = f'''date_Issued: \"{self.date_issued}\"'''\n",
    "        self.yaml_row_3 = f'''date_Issued_edtf: \"{self.date_issued_edtf}\"'''\n",
    "        self.yaml_rows_list = [self.yaml_row_0, self.yaml_row_1, self.yaml_row_2, self.yaml_row_3]\n",
    "        \n",
    "    def create_yaml(self):\n",
    "        \n",
    "        if self.yaml_path.is_file():\n",
    "            print(f'{self.yaml_path} already exists')\n",
    "            raise FileExistsError\n",
    "            return\n",
    "        else:  # create it\n",
    "            print(f'Creating {self.yaml_path}')\n",
    "            with open(self.yaml_path, 'a+') as yml_file:\n",
    "                for yaml_row in self.yaml_rows_list:\n",
    "                    yml_file.write(f'{yaml_row}\\n')  # add line break\n",
    "            # !touch \"{self.yaml_path}\"\n",
    "            # for yaml_row in self.yaml_rows_list:\n",
    "            #     print(f'Adding {yaml_row}')\n",
    "            #     !echo \"{yaml_row}\" >> \"{self.yaml_path}\"\n",
    "            # print(f'YAML data in {self.yaml_path}')\n",
    "            !cat \"{self.yaml_path}\"\n",
    "            return\n",
    "        \n",
    "    def process_publication(self):\n",
    "        \n",
    "        # rename files\n",
    "        self.rename_files_to_directory_name('.tiff')  # process .tiff first just in case\n",
    "        self.rename_files_to_directory_name('.tif')  # then make sure it's all .tif\n",
    "        \n",
    "        # create YAML file, ingest directory\n",
    "        self.create_yaml()\n",
    "        ingest_directory_path = self.create_islandora_ingest_directory()\n",
    "        \n",
    "        # create Islandora-required book directory then move ingest directory into it\n",
    "        self.book_directory_path = self.directory_path.parents[0].joinpath('book')\n",
    "        self.book_directory_path.mkdir(exist_ok=True)\n",
    "        self.final_path = self.book_directory_path.joinpath(ingest_directory_path.name)\n",
    "        ingest_directory_path.replace(self.final_path)\n",
    "        \n",
    "        # move YAML file into book directory\n",
    "        new_yaml_path = self.book_directory_path.joinpath(self.yaml_path.name)\n",
    "        self.yaml_path.replace(new_yaml_path) \n",
    "\n",
    "        number_of_books = len([x for x in self.book_directory_path.iterdir() if x.is_dir()])\n",
    "        print(f'{number_of_books} books in {self.book_directory_path} for ingest')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory_path = Path('/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills')\n",
    "adminDB_collection = 3049\n",
    "adminDB_next_item = 873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7 directories\n",
      "/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2013-10-03_Our_Country's_Good\n",
      "0 with .tiff\n",
      "0 files to process\n",
      "6 with .tif\n",
      "Backing up 2013-10-03_Our_Country's_Good . . .\n",
      "Backup created\n",
      "Renaming 6 \".tif\"s in 2013-10-03_Our_Country's_Good . . .\n",
      " Renamed 6 \".tif\"s\n",
      "\n",
      "Creating /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2013-10-03_Our_Country's_Good.yml\n",
      "adminDB: \"0012_003049_000873\"\n",
      "Title: \"Our Country's Good\"\n",
      "date_Issued: \"October 3, 2013\"\n",
      "date_Issued_edtf: \"2013-10-03\"\n",
      "Processing 6 images in 2013-10-03_Our_Country's_Good\n",
      "Renaming ._2013-10-03_Our_Country's_Good_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2013-10-03_Our_Country's_Good/ORIGINAL.pdf\n",
      "\n",
      "Renaming 2013-10-03_Our_Country's_Good_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2013-10-03_Our_Country's_Good/ORIGINAL.pdf\n",
      "\n",
      "Ingest directory created at /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2013-10-03_Our_Country's_Good\n",
      "\n",
      "1 books in /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/book for ingest\n",
      "\n",
      "/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2013-10-24_CTRL+ALT+DELETE\n",
      "0 with .tiff\n",
      "0 files to process\n",
      "6 with .tif\n",
      "Backing up 2013-10-24_CTRL+ALT+DELETE . . .\n",
      "Backup created\n",
      "Renaming 6 \".tif\"s in 2013-10-24_CTRL+ALT+DELETE . . .\n",
      " Renamed 6 \".tif\"s\n",
      "\n",
      "Creating /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2013-10-24_CTRL+ALT+DELETE.yml\n",
      "adminDB: \"0012_003049_000874\"\n",
      "Title: \"CTRL+ALT+DELETE\"\n",
      "date_Issued: \"October 24, 2013\"\n",
      "date_Issued_edtf: \"2013-10-24\"\n",
      "Processing 6 images in 2013-10-24_CTRL+ALT+DELETE\n",
      "Renaming 2013-10-24_CTRL+ALT+DELETE_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2013-10-24_CTRL+ALT+DELETE/ORIGINAL.pdf\n",
      "\n",
      "Ingest directory created at /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2013-10-24_CTRL+ALT+DELETE\n",
      "\n",
      "2 books in /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/book for ingest\n",
      "\n",
      "/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-01-30_The_Whipping_Man\n",
      "0 with .tiff\n",
      "0 files to process\n",
      "6 with .tif\n",
      "Backing up 2014-01-30_The_Whipping_Man . . .\n",
      "Backup created\n",
      "Renaming 6 \".tif\"s in 2014-01-30_The_Whipping_Man . . .\n",
      " Renamed 6 \".tif\"s\n",
      "\n",
      "Creating /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-01-30_The_Whipping_Man.yml\n",
      "adminDB: \"0012_003049_000875\"\n",
      "Title: \"The Whipping Man\"\n",
      "date_Issued: \"January 30, 2014\"\n",
      "date_Issued_edtf: \"2014-01-30\"\n",
      "Processing 6 images in 2014-01-30_The_Whipping_Man\n",
      "Renaming ._2014-01-30_The_Whipping_Man_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-01-30_The_Whipping_Man/ORIGINAL.pdf\n",
      "\n",
      "Renaming 2014-01-30_The_Whipping_Man_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-01-30_The_Whipping_Man/ORIGINAL.pdf\n",
      "\n",
      "Ingest directory created at /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-01-30_The_Whipping_Man\n",
      "\n",
      "3 books in /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/book for ingest\n",
      "\n",
      "/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-02-20_The_Trip_to_Bountiful\n",
      "0 with .tiff\n",
      "0 files to process\n",
      "6 with .tif\n",
      "Backing up 2014-02-20_The_Trip_to_Bountiful . . .\n",
      "Backup created\n",
      "Renaming 6 \".tif\"s in 2014-02-20_The_Trip_to_Bountiful . . .\n",
      " Renamed 6 \".tif\"s\n",
      "\n",
      "Creating /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-02-20_The_Trip_to_Bountiful.yml\n",
      "adminDB: \"0012_003049_000876\"\n",
      "Title: \"The Trip to Bountiful\"\n",
      "date_Issued: \"February 20, 2014\"\n",
      "date_Issued_edtf: \"2014-02-20\"\n",
      "Processing 6 images in 2014-02-20_The_Trip_to_Bountiful\n",
      "Renaming 2014-02-20_The_Trip_to_Bountiful_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-02-20_The_Trip_to_Bountiful/ORIGINAL.pdf\n",
      "\n",
      "Ingest directory created at /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-02-20_The_Trip_to_Bountiful\n",
      "\n",
      "4 books in /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/book for ingest\n",
      "\n",
      "/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-03-27_WRENS\n",
      "0 with .tiff\n",
      "0 files to process\n",
      "6 with .tif\n",
      "Backing up 2014-03-27_WRENS . . .\n",
      "Backup created\n",
      "Renaming 6 \".tif\"s in 2014-03-27_WRENS . . .\n",
      " Renamed 6 \".tif\"s\n",
      "\n",
      "Creating /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-03-27_WRENS.yml\n",
      "adminDB: \"0012_003049_000877\"\n",
      "Title: \"WRENS\"\n",
      "date_Issued: \"March 27, 2014\"\n",
      "date_Issued_edtf: \"2014-03-27\"\n",
      "Processing 6 images in 2014-03-27_WRENS\n",
      "Renaming ._2014-03-27_WRENS_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-03-27_WRENS/ORIGINAL.pdf\n",
      "\n",
      "Renaming 2014-03-27_WRENS_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-03-27_WRENS/ORIGINAL.pdf\n",
      "\n",
      "Ingest directory created at /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-03-27_WRENS\n",
      "\n",
      "5 books in /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/book for ingest\n",
      "\n",
      "/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-04-24_Spamalot\n",
      "0 with .tiff\n",
      "0 files to process\n",
      "6 with .tif\n",
      "Backing up 2014-04-24_Spamalot . . .\n",
      "Backup created\n",
      "Renaming 6 \".tif\"s in 2014-04-24_Spamalot . . .\n",
      " Renamed 6 \".tif\"s\n",
      "\n",
      "Creating /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-04-24_Spamalot.yml\n",
      "adminDB: \"0012_003049_000878\"\n",
      "Title: \"Spamalot\"\n",
      "date_Issued: \"April 24, 2014\"\n",
      "date_Issued_edtf: \"2014-04-24\"\n",
      "Processing 6 images in 2014-04-24_Spamalot\n",
      "Renaming 2014-04-24_Spamalot_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-04-24_Spamalot/ORIGINAL.pdf\n",
      "\n",
      "Ingest directory created at /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-04-24_Spamalot\n",
      "\n",
      "6 books in /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/book for ingest\n",
      "\n",
      "/Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-11-27_A_Christmas_Carol\n",
      "0 with .tiff\n",
      "0 files to process\n",
      "6 with .tif\n",
      "Backing up 2014-11-27_A_Christmas_Carol . . .\n",
      "Backup created\n",
      "Renaming 6 \".tif\"s in 2014-11-27_A_Christmas_Carol . . .\n",
      " Renamed 6 \".tif\"s\n",
      "\n",
      "Creating /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-11-27_A_Christmas_Carol.yml\n",
      "adminDB: \"0012_003049_000879\"\n",
      "Title: \"A Christmas Carol\"\n",
      "date_Issued: \"November 27, 2014\"\n",
      "date_Issued_edtf: \"2014-11-27\"\n",
      "Processing 6 images in 2014-11-27_A_Christmas_Carol\n",
      "Renaming 2014-11-27_A_Christmas_Carol_original.pdf to /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-11-27_A_Christmas_Carol/ORIGINAL.pdf\n",
      "\n",
      "Ingest directory created at /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/2014-11-27_A_Christmas_Carol\n",
      "\n",
      "7 books in /Volumes/fluffy/ContinuingPublications/BacklogApril2019/0.toProcessForUpload/Playbills/book for ingest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_process_playbills(root_directory_path, adminDB_collection, adminDB_next_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rename PDFs to match directory name with \"_original\" at the end\n",
    "directory_paths_list = [x for x in root_directory_path.iterdir() if x.is_dir()]\n",
    "\n",
    "for directory_path in directory_paths_list:\n",
    "    print('')\n",
    "    print(f'Directory: {directory_path}')\n",
    "    print('')\n",
    "\n",
    "    pdf_path = list(directory_path.glob('*.pdf'))[0]\n",
    "    new_pdf_path = directory_path.joinpath(f'{directory_path.name}_original.pdf')\n",
    "    pdf_path.rename(new_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# just rename images to match directory name\n",
    "directory_paths_list = [x for x in root_directory_path.iterdir() if x.is_dir()]\n",
    "\n",
    "for directory_path in directory_paths_list:\n",
    "    print('')\n",
    "    print(f'Directory: {directory_path}')\n",
    "    print('')\n",
    "\n",
    "    # create Volume\n",
    "    volume = ContinuingPublications_Volume(directory_path)\n",
    "\n",
    "    # rename Adobe Acrobat .tiff files to directory and .tif extension\n",
    "    volume.rename_tiffs_to_directory_name('.tiff')\n",
    "    volume.rename_tiffs_to_directory_name('.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true,
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# run ContinuingPub class's directory creation algorithm\n",
    "directory_paths_list = [x for x in root_directory_path.iterdir() if x.is_dir()]\n",
    "\n",
    "for directory_path in directory_paths_list:\n",
    "    print('')\n",
    "    print(f'Directory: {directory_path}')\n",
    "    print('')\n",
    "\n",
    "    # create Volume\n",
    "    volume = ContinuingPublications_Volume(directory_path)\n",
    "\n",
    "    # rename Adobe Acrobat .tiff and .tif files to directory name\n",
    "    volume.rename_tiffs_to_directory_name('.tiff')\n",
    "    volume.rename_tiffs_to_directory_name('.tif')\n",
    "\n",
    "    # rename PDFs for ingest\n",
    "    volume.rename_PDFs_for_ingest()\n",
    "\n",
    "    # create Islanodra book ingest directory\n",
    "    ingest_directory_path = volume.create_islandora_ingest_directory()\n",
    "\n",
    "    # create book directory path as needed for Islandora\n",
    "    book_directory_path = volume.directory_path.parents[0].joinpath('book')\n",
    "    book_directory_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # move ingest directory into book directory\n",
    "    final_path = book_directory_path.joinpath(ingest_directory_path.name)\n",
    "    ingest_directory_path.replace(final_path)\n",
    "\n",
    "    number_of_books = len([x for x in book_directory_path.iterdir() if x.is_dir()])\n",
    "    print(f'{number_of_books} books in {book_directory_path} for ingest')\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "directory_paths_list = [x for x in root_directory_path.iterdir() if x.is_dir()]\n",
    "dir_test = directory_paths_list[-1]\n",
    "dir_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = Playbills(dir_test, adminDB_collection, adminDB_next_item)\n",
    "volume.title, volume.title_replace_underscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.date, volume.yyyy, volume.mm, volume.month, volume.dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.date_issued, volume.date_issued_edtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.adminDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.yaml_path, volume.directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in volume.yaml_rows_list:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = Playbills(dir_test, adminDB_collection, adminDB_next_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories_list = list(volume.directory_path.glob('**'))\n",
    "directories_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "everything_paths_list = list(volume.directory_path.glob('**/*'))\n",
    "everything_paths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_backup_dir = volume.directory_path.parent.joinpath(f'{volume.directory_path.name}_backup')\n",
    "test_backup_dir.mkdir()\n",
    "print(test_backup_dir.is_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_backup_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs_list = [x for x in everything_paths_list if x.is_dir()]\n",
    "files_list = [x for x in everything_paths_list if x.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_path in dirs_list:\n",
    "    # get dir_path without volume.directory_path at the beginning\n",
    "    local_path = str(dir_path).replace(str(volume.directory_path), '')\n",
    "    # get a list of the directories by stripping the forward-slashes\n",
    "    local_path = local_path.strip('/')\n",
    "    # create output_dir and create it\n",
    "    output_dir = test_backup_dir.joinpath(local_path)\n",
    "    output_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in files_list:\n",
    "    # get local path for file (everything after volume.directory_path) as a list\n",
    "    local_path = str(file_path).replace(str(volume.directory_path), '').strip('/')\n",
    "    # set output_path and copy\n",
    "    output_path = test_backup_dir.joinpath(local_path)\n",
    "    copy2(file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_hash = md5_dir(volume.directory_path)\n",
    "backup_hash = md5_dir(test_backup_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_hash)\n",
    "print(backup_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.get_file_paths('.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
